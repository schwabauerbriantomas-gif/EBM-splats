# EBM (Energy-Based Model) para Lenguaje

[![Status](https://img.shields.io/badge/status-entrenando-yellow.svg)](https://github.com)
[![Vulkan](https://img.shields.io/badge/vulkan-1.3-red.svg)](https://vulkan.org)
[![Python](https://img.shields.io/badge/python-3.10%2B-brightgreen.svg)](https://python.org)
[![License](https://img.shields.io/badge/license-Apache%202.0-blue.svg)](LICENSE)
[![GPU](https://img.shields.io/badge/GPU-AMD%20RX%206650XT-orange.svg)](https://amd.com)

> **Energy-Based Model para generaciÃ³n de lenguaje sobre hiperesfera 640D con Gaussian Splats como atractores y dinÃ¡mica Langevin para sampleo.**

---

## ğŸ“‹ Tabla de Contenidos

- [Estado del Proyecto](#-estado-del-proyecto)
- [Arquitectura](#-arquitectura)
- [Avances Logrados](#-avances-logrados)
- [Limitaciones y Defectos Actuales](#-limitaciones-y-defectos-actuales)
- [Quick Start](#-quick-start)
- [DocumentaciÃ³n TÃ©cnica](#-documentaciÃ³n-tÃ©cnica)
- [Roadmap](#-roadmap)

---

## ğŸ¯ Estado del Proyecto

**VersiÃ³n**: 2.0 - ImplementaciÃ³n Composicional
**Estado**: ğŸ”„ **En entrenamiento activo** (Vulkan GPU acceleration)
**Inicio**: Febrero 2026
**UbicaciÃ³n**: `projects/ebm/`

### Validaciones Completadas âœ…

| ValidaciÃ³n | Estado | DescripciÃ³n |
|------------|--------|-------------|
| **Geometric Correctness** | âœ… PASS | Mapeo exacto a S^639 |
| **Training Stability** | âœ… PASS | 16-token dummy sequence |
| **Text Generation** | âœ… PASS | Langevin sample sin NaN |
| **Dataset Integration** | âœ… PASS | wikitext-103 + GPT-2 tokenizer |
| **Vulkan Dispatch** | âœ… PASS | Riemannian scores idÃ©nticos |

### Progreso de Entrenamiento ğŸ”„

- **Dataset**: wikitext-103 (20K samples, 5116 batches/epoch)
- **Epochs**: 10 planificados
- **Batch size**: 16
- **Estado**: Entrenando en background
- **Checkpoints**: `checkpoints/ebm_epoch_X.pt`

---

## ğŸ— Arquitectura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EBM Architecture (S^639)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  Input â†’ Tokenizer (GPT-2) â†’ Embedding (640D)                       â”‚
â”‚                                                                      â”‚
â”‚  Embedding â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚              â”‚  SplatStore  â”‚ â†’ Gaussian Splats (Î¼, Î±, Îº)           â”‚
â”‚              â”‚   (50K max)  â”‚                                        â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                      â†“                                               â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚              â”‚ Energy Func  â”‚ â†’ E(x) = E_splats + E_geom + E_comp   â”‚
â”‚              â”‚  (Riemann)   â”‚                                        â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                      â†“                                               â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚              â”‚  Langevin    â”‚ â†’ Underdamped Dynamics (200 steps)    â”‚
â”‚              â”‚  Sampler     â”‚                                        â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                      â†“                                               â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚              â”‚  SOC Ctrl    â”‚ â†’ Self-Organized Criticality          â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                      â†“                                               â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                        â”‚
â”‚              â”‚  MoE Decoder â”‚ â†’ 4 Experts, 2 Active                 â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                        â”‚
â”‚                      â†“                                               â”‚
â”‚  Output â† Tokens â† Logits                                           â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes Principales

| Componente | Archivo | DescripciÃ³n |
|------------|---------|-------------|
| **Tokenizer** | `dataset_utils.py` | GPT-2 tokenizer (vocab: 50,257) |
| **SplatStore** | `splats.py` | ImprovedSplatStore con KNN FAISS |
| **EnergyFunction** | `energy.py` | Splat + GeomÃ©trica + Composicional |
| **Langevin** | `langevin.py` | Underdamped StÃ¶rmer-Verlet integrator |
| **SOC Controller** | `soc.py` | HistoryBuffer + consolidaciÃ³n automÃ¡tica |
| **Decoder** | `decoder.py` | Mixture of Experts (4 expertos, 2 activos) |
| **Geometry** | `geometry.py` | Operaciones Riemannianas (exp_map, log_map) |
| **Vulkan Engine** | `vulkan_engine.py` | GPU acceleration para AMD RX 6650XT |

---

## âœ… Avances Logrados

### Fase 1: Convergencia y ValidaciÃ³n (Completada)

#### 1. InicializaciÃ³n Inteligente de Splats âœ…
- **Cargar embeddings GPT-2 pre-entrenadas** para representaciÃ³n semÃ¡ntica inicial
- **Expandir de 10K a 50K splats** progresivamente con curriculum learning
- **Temperatura de energy configurada** para mejor exploraciÃ³n

**Impacto**: Cobertura de vocabulario mejorada significativamente

#### 2. Curriculum Learning âœ…
- **Fase 1**: 5K splats, alta temperatura
- **Fase 2**: 30K splats, temperatura media
- **Fase 3**: 50K splats, fine-tuning

**Impacto**: Progreso mÃ¡s estable y predecible

#### 3. Monitoreo Avanzado âœ…
- **MÃ©tricas en vivo**: Loss, energÃ­a, estadÃ­sticas de splats, SOC rate
- **Logging detallado**: Timestamps, checkpoints cada 5 epochs
- **Alertas automÃ¡ticas**: EnergÃ­a aumentando, SOC demasiado rÃ¡pido

**Impacto**: DetecciÃ³n temprana de problemas

#### 4. ValidaciÃ³n AutomÃ¡tica âœ…
- **EvaluaciÃ³n de checkpoints**: Perplexity, mÃ©tricas de energÃ­a
- **Herramientas de diagnÃ³stico**: `diagnose.py`, `evaluate.py`
- **Muestras generadas**: EvaluaciÃ³n humana

**Impacto**: Feedback en tiempo real sobre calidad

#### 5. Mejoras de Splat Store âœ…
- **EstadÃ­sticas completas**: Frecuencia, edad, kappa dinÃ¡mico
- **Weight decay gradual**: Por epoch
- **LÃ­mites configurables**: kappa âˆˆ [1.0, 50.0]

**Impacto**: Mejor gestiÃ³n de recursos de splats

---

## âš ï¸ Limitaciones y Defectos Actuales

### ğŸ”´ CrÃ­ticos

#### 1. Tiempo de Convergencia
**Problema**: Entrenamiento requiere dÃ­as/semanas en GPU local

> *"GPT-2 level functionality inherently traces hundreds of millions of parameters over enormous server-grade GPU clusters for several weeks. Translating this quality identically down onto a single continuous discrete RX 6650XT Vulkan mapping means that the pretrain.py instance currently running should be left undisturbed for several days (or weeks)."*

**MitigaciÃ³n**:
- âœ… Curriculum learning implementado
- âœ… Checkpoints cada epoch para resumir
- ğŸ”„ Monitoreo continuo de progreso

**Estado**: Aceptado como limitaciÃ³n de hardware

---

#### 2. BÃºsqueda de Splats O(N)
**Problema**: KNN con FAISS-CPU es O(N), no O(log N)

**Impacto**: BÃºsqueda se vuelve lenta con muchos splats (50K+)

**MitigaciÃ³n**:
- âœ… FAISS-CPU implementado (12x speedup vs naive)
- ğŸ”„ Pendiente: FAISS-GPU migration

**SoluciÃ³n Futura**: HRM2 hierarchical search (como M2M)

---

#### 3. Embeddings Hash-Based (Demo)
**Problema**: Ãndice actual usa embeddings hash-based, no semÃ¡nticos

**Impacto**: BÃºsqueda no captura semÃ¡ntica real

**MitigaciÃ³n**:
- ğŸ”„ TODO: Integrar sentence-transformers

**Estado**: LimitaciÃ³n conocida del prototipo

---

### ğŸŸ¡ Moderados

#### 4. Batch Size Limitado
**Problema**: Batch size = 16 (limitado por VRAM de 8GB)

**Impacto**: Entrenamiento mÃ¡s lento, gradientes menos estables

**MitigaciÃ³n**:
- ğŸ”„ TODO: Mixed precision training (BF16)
- ğŸ”„ TODO: Gradient accumulation (effective batch 8x)

---

#### 5. Decoder Simplificado
**Problema**: MoE decoder es ligero (4 expertos, 2 activos)

**Impacto**: Calidad de generaciÃ³n puede ser inferior a transformers grandes

**MitigaciÃ³n**:
- âœ… Arquitectura funcional
- ğŸ”„ TODO: Transformer decoder estilo GPT-2

---

#### 6. Sin IntegraciÃ³n LLM Completa
**Problema**: EBM genera tokens pero no estÃ¡ integrado con LLM externo

**Impacto**: No se puede usar en pipelines RAG directamente

**MitigaciÃ³n**:
- ğŸ”„ TODO: IntegraciÃ³n con LangChain/LlamaIndex
- ğŸ”„ TODO: API REST para uso externo

---

### ğŸŸ¢ Menores

#### 7. Logging Detallado pero Verbose
**Problema**: Logs pueden ser muy extensos

**MitigaciÃ³n**: âœ… Niveles de logging configurables

---

#### 8. Dependencia de Vulkan SDK
**Problema**: Requiere instalaciÃ³n manual de Vulkan SDK

**MitigaciÃ³n**: âœ… Fallback a CPU si Vulkan no estÃ¡ disponible

---

## ğŸš€ Quick Start

### Requisitos

```bash
# Dependencias principales
pip install torch numpy transformers datasets faiss-cpu

# Vulkan SDK (opcional, para GPU acceleration)
# https://vulkan.lunarg.com/
```

### Entrenar

```bash
# GPU (Recomendado)
python train.py --device vulkan --epochs 10 --batch-size 16

# CPU (Lento)
python train.py --device cpu --epochs 10 --batch-size 16

# Reanudar desde checkpoint
python train.py --device vulkan --resume checkpoints/ebm_epoch_5.pt
```

### Diagnosticar

```bash
# AnÃ¡lisis de checkpoint especÃ­fico
python diagnose.py --checkpoint checkpoints/ebm_epoch_5.pt --device vulkan

# AnÃ¡lisis batch de todos los checkpoints
python diagnose.py --batch --device vulkan

# Generar reporte con recomendaciones
python diagnose.py --checkpoint checkpoints/ebm_epoch_10.pt --report
```

### Evaluar

```bash
# Calcular perplexity en WikiText-103
python evaluate.py --checkpoint checkpoints/ebm_epoch_10.pt --device vulkan

# Generar muestras
python generate.py --checkpoint checkpoints/ebm_epoch_10.pt --prompt "The future of AI"
```

---

## ğŸ“– DocumentaciÃ³n TÃ©cnica

### EspecificaciÃ³n Completa
- **Archivo**: `spec.txt`
- **Contenido**: 20 secciones, 620+ lÃ­neas
- **Incluye**: FÃ³rmulas matemÃ¡ticas completas, hiperparÃ¡metros, pipeline completo

### Espacio Latente

| Propiedad | Valor |
|-----------|-------|
| **Manifold** | S^639 (hiperesfera unitaria) |
| **DimensiÃ³n** | 640D |
| **RestricciÃ³n** | \|\|x\|\|Â² = 1 |
| **MÃ©trica** | g_x = I - xÂ·x^T |
| **Distancia** | d(x,y) = arccos(xÂ·y) |

### Gaussian Splats

| ParÃ¡metro | DescripciÃ³n | Rango |
|-----------|-------------|-------|
| **Î¼** | Media direccional [640] | Esfera unitaria |
| **Î±** | Peso/intensidad | (0, âˆ) |
| **Îº** | ConcentraciÃ³n | [1.0, 50.0] |

### Langevin Underdamped

```
dx/dt = v
dv/dt = -Î³v - âˆ‡_R E(x) + âˆš(2Î³T)Â·Î¾
```

| ParÃ¡metro | Valor |
|-----------|-------|
| **Pasos** | 200 |
| **dt** | 0.001 |
| **FricciÃ³n (Î³)** | 0.1 |
| **Temperatura (T)** | 1.0 |

### Entrenamiento

| ParÃ¡metro | Valor |
|-----------|-------|
| **MÃ©todo** | Denoising Score Matching |
| **Loss** | L = E[\|\|s_Î¸(xÌƒ) - Îµ/Ïƒ\|\|Â²] |
| **Dataset** | wikitext-103 |
| **Batch size** | 16 |
| **Learning rate** | 1e-4 (Cosine Annealing) |
| **Noise levels** | (0.01, 0.05, 0.1, 0.2, 0.5) |

---

## ğŸ—º Roadmap

### âœ… Completado

- [x] Arquitectura base EBM
- [x] Gaussian Splats con KNN
- [x] Langevin Underdamped
- [x] SOC Controller
- [x] Vulkan GPU acceleration
- [x] Curriculum Learning
- [x] Monitoreo avanzado
- [x] DiagnÃ³stico automÃ¡tico
- [x] ValidaciÃ³n geomÃ©trica

### ğŸ”„ En Progreso

- [ ] Entrenamiento completo (10 epochs)
- [ ] EvaluaciÃ³n de perplexity
- [ ] AnÃ¡lisis de convergencia

### ğŸ“‹ Futuro (Fase 2 - Opcional)

- [ ] **FAISS-GPU Migration**: AceleraciÃ³n real de KNN
- [ ] **Mixed Precision Training**: BF16 para 2x capacidad
- [ ] **Gradient Accumulation**: Effective batch 8x
- [ ] **Transformer Decoder**: Arquitectura GPT-2
- [ ] **HRM2 Integration**: BÃºsqueda O(log N)
- [ ] **API REST**: IntegraciÃ³n con sistemas externos
- [ ] **LangChain/LlamaIndex**: Pipelines RAG

---

## ğŸ“Š MÃ©tricas de Ã‰xito

### Targets Fase 1

| MÃ©trica | Target | Estado |
|---------|--------|--------|
| **Perplexity (WikiText)** | < 100 | ğŸ”„ Por validar |
| **Energy Trend** | Decreciente | ğŸ”„ Monitoreando |
| **Splat Coverage** | > 80% | ğŸ”„ Por medir |
| **SOC Rate** | Decreciente | ğŸ”„ Monitoreando |

### MÃ©tricas de Convergencia

| Indicador | Excelente | Bueno | Regular | Malo |
|-----------|-----------|-------|---------|------|
| **Loss Score Matching** | < 0.05 | < 0.1 | < 0.2 | > 0.2 |
| **EnergÃ­a Promedio** | Decreciente | Estable | Fluctuante | Creciente |
| **Tendencia** | Converging | Stable | Needs attention | Diverging |

---

## ğŸ¤ Contribuir

### Estructura del Proyecto

```
projects/ebm/
â”œâ”€â”€ train.py              # Script principal de entrenamiento
â”œâ”€â”€ diagnose.py           # DiagnÃ³stico de checkpoints
â”œâ”€â”€ evaluate.py           # EvaluaciÃ³n de calidad
â”œâ”€â”€ generate.py           # GeneraciÃ³n de texto
â”œâ”€â”€ model.py              # EBMModel principal
â”œâ”€â”€ splats.py             # ImprovedSplatStore
â”œâ”€â”€ energy.py             # EnergyFunction
â”œâ”€â”€ langevin.py           # Langevin sampler
â”œâ”€â”€ soc.py                # SOC controller
â”œâ”€â”€ decoder.py            # MoE decoder
â”œâ”€â”€ geometry.py           # Operaciones Riemannianas
â”œâ”€â”€ vulkan_engine.py      # GPU acceleration
â”œâ”€â”€ config.py             # ConfiguraciÃ³n
â”œâ”€â”€ dataset_utils.py      # WikiText-103 dataloader
â”œâ”€â”€ spec.txt              # EspecificaciÃ³n tÃ©cnica completa
â””â”€â”€ README.md             # Este archivo
```

### Dependencias

Ver `requirements.txt` para lista completa.

---

## ğŸ“š Referencias

- **EspecificaciÃ³n tÃ©cnica**: `spec.txt`
- **DocumentaciÃ³n M2M**: `../m2m/README.md`
- **IntegraciÃ³n M2M-EBM**: `../../MEMORY.md`

---

## ğŸ“„ Licencia

Apache License 2.0

---

## ğŸ‘¤ Autor

**Alfred** ğŸ© - Asistente AI del Sr. Schwabauer

---

## ğŸ™ Agradecimientos

- **DeepSeek**: InspiraciÃ³n para Engram memory
- **Gaussian Splatting**: Foundation para representaciones
- **Vulkan SDK**: GPU acceleration

---

**Ãšltima actualizaciÃ³n**: 2026-02-23
**VersiÃ³n**: 2.0
**Estado**: En entrenamiento activo ğŸ”„

---

> *"El objetivo no es artificial general intelligence â€” es genuine specific usefulness."*
