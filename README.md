EBM (Energy-Based Model) for Language
Energy-Based Model for language generation on a 640D hypersphere, with Gaussian Splats as attractors and Langevin dynamics for sampling.

Status: üîÑ In active training with Phase 1 EnhancementsLocation: projects/ebm/Start: February 2026

üéâ New Implemented Enhancements (Phase 1)
‚úÖ Phase 1: Convergence and Validation
1. Smart Splat Initialization
Load pre-trained GPT-2 embeddings for rich initial semantic representation
Expand from 10K to 50K splats progressively with curriculum learning
Configured energy temperature for better initial exploration
Benefits:

Much better vocabulary coverage from the start
Semantically meaningful initial representations
Drastic reduction in convergence time
2. Curriculum Learning
Phase 1 (Init): 5K splats, learn basic representations, high temperature
Phase 2 (Mid): 30K splats, expand vocabulary, medium temperature
Phase 3 (Max): 50K splats, full fine-tuning, low temperature
Benefits:

More predictable and stable progress
Avoids local collapse in minima
Better usage of GPU capacity per phase
3. Advanced Monitoring
Live Metrics:
Score matching loss (per batch and per epoch)
Average energy with trend
Splat statistics (n_active, frequency, age)
SOC consolidation rate
Perplexity on WikiText-103 validation
Detailed Logging:
Exact timestamps per batch
Checkpoints every 5 epochs (in addition to every epoch)
Diagnostic information (n_splats, average distances)
Automatic Alerts:
Energy increasing unexpectedly
SOC consolidating too quickly
Perplexity worsening
Poor convergence detected
4. Automatic Validation
Checkpoint Evaluation:
Automatic Perplexity on validation subset
Energy metrics per epoch
Convergence trend analysis
Diagnostic Tools:
diagnose.py: Automatic checkpoint analysis
evaluate.py: Generative quality metrics
Generated samples for human evaluation
5. Splat Store Enhancements
Improved splat statistics:
Usage frequency tracking
Age of each splat for weight decay
Dynamic Kappa with configurable limits (min: 1.0, max: 50.0)
Temperature adjustment for more exploration
Gradual weight decay per epoch
üèó Model Architecture
Tokenizer ‚Üí Embedding ‚Üí Splat Store ‚Üí Energy ‚Üí Langevin ‚Üí Decoder ‚Üí Tokens
(Œº, Œ±, Œ∫) (Riemann) (MoE)

text


**Enhanced Components**:
- **ImprovedSplatStore**: Up to 50K splats with FAISS-CPU KNN
- **EnergyFunction**: Splat + Geometric + Compositional
- **Langevin Dynamics**: Underdamped (momentum) with 200 steps
- **SOC Controller**: Self-Organized Criticality for consolidation
- **EBMDecoder**: Mixture of Experts (4 experts, 2 active)
- **Geometry**: Full Riemannian operations (exp_map, log_map, gradient projection)

---

## üöÄ How to Train

### Quick Start (Vulkan GPU)

```bash
# Train with Phase 1 enhancements using GPU
python train.py --device vulkan --epochs 10 --batch-size 32

# Resume from checkpoint
python train.py --device vulkan --resume

# Validate existing checkpoint
python diagnose.py --checkpoint checkpoints/ebm_epoch_5.pt --device vulkan
Automatic Diagnosis
bash

# Detailed analysis of specific checkpoint
python diagnose.py --checkpoint checkpoints/ebm_epoch_X.pt --device vulkan

# Batch analysis of all checkpoints
python diagnose.py --batch --device vulkan

# Generate report with recommendations
python diagnose.py --checkpoint checkpoints/ebm_epoch_10.pt --device vulkan --report
---

üìä Success Metrics
Phase 1 Objectives
Metric
Target
Progress
Perplexity (WikiText)	< 100	Pending validation
Energy Trend	Stable/Decreasing	To be measured in training
Splat Coverage	80%+	Pending measurement
SOC Rate	Decreasing	To be measured in training

Convergence Metrics
Score Matching Loss: Target < 0.1
Average Energy: Stable and decreasing
Trend: Converging or Excellent (stable)
Consolidation Rate: Decreasing over time
üìÅ Project Files
Core Architecture
config.py - Centralized configuration (EbmConfig dataclass)
model.py - Main EBMModel
splats.py - ImprovedSplatStore (50K splats with KNN)
energy.py - EnergyFunction (Splat + Geometric + Compositional)
langevin.py - Underdamped Langevin sampler
soc.py - HistoryBuffer + SOC consolidation
decoder.py - EBMDecoder (MoE: 4 experts, 2 active)
geometry.py - Full Riemannian operations
Training and Evaluation
train.py - Main training script with Phase 1 enhancements ‚úÖ IMPROVED
evaluate.py - Perplexity and generative quality evaluation ‚úÖ NEW
diagnose.py - Automatic checkpoint diagnostics ‚úÖ NEW
pretrain.py - Existing pretraining script
train_logger.py - Detailed training logging ‚úÖ NEW
Utilities
dataset_utils.py - WikiText-103 dataloader
vulkan_engine.py - VulkanEBMRunner (GPU acceleration)
config.py - Fallback configuration
Documentation
README.md - This file with complete instructions ‚úÖ NEW
requirements.txt - Project dependencies ‚úÖ NEW
Checkpoints and Logs
checkpoints/ - Model checkpoints saved every epoch
logs/ebm/ - Detailed training logs (JSON)
üéØ Differences from Original Design
Aspect
Original
Phase 1 Enhanced
Benefit
Splats Init	10K random	50K GPT-2 embeddings	Better semantic coverage
Training	Single phase	3-phase curriculum	More stable convergence
Monitoring	Basic	Live metrics + alerts	Problems detected early
Validation	Manual	Automatic with diagnostics	Real-time feedback
Splat Stats	Simple	Complete statistics	Better model understanding

üìñ Documentation
Quick Start
bash

# Install dependencies
pip install -r requirements.txt

# Train with GPU (Recommended for AMD RX 6650XT)
python train.py --device vulkan --epochs 10 --batch-size 32

# Monitor live training
# Logs are saved in logs/ebm/training_log_TIMESTAMP.json
Diagnosis
bash

# Specific checkpoint analysis
python diagnose.py --checkpoint checkpoints/ebm_epoch_5.pt --device vulkan

# Batch diagnosis of all checkpoints
python diagnose.py --batch --device vulkan
üöÄ Next Steps (Phase 2 - Optional)
These enhancements will only be implemented if Phase 1 ones do not solve convergence problems:

FAISS-GPU Migration: Real acceleration of splat KNN
Mixed Precision Training: BF16 for 2x batch capacity
Gradient Accumulation: Effective batch size 8x (currently 1)
Transformer Decoder: Proven GPT-2 style architecture
Hierarchical Sampling: Coarse-to-fine for greater efficiency
üîß Configuration
Phase 1 Parameters (config.py)
python

@dataclass
class EBMConfig:
    # Environment
    device: str = "vulkan"  # Use AMD RX 6650XT GPU

    # Latent space
    latent_dim: int = 640

    # Splats (Phase 1 enhanced)
    n_splats_init: int = 10000  # Initial: 10K, then expand to 50K
    max_splats: int = 150000  # Maximum capacity: 50K
    knn_k: int = 64

    # Curriculum learning (Phase 1 new)
    enable_curriculum_learning: bool = True
    curriculum_epochs: int = 5
    curriculum_target_splats: int = 50000

    # Monitoring (Phase 1 enhanced)
    enable_detailed_logging: bool = True
    soc_check_interval: int = 100

    # Splat regularization (Phase 1 enhanced)
    splat_temperature: float = 0.1
    splat_weight_decay: float = 0.0
    splat_weight_decay_start: float = 1.0
    min_kappa: float = 1.0
    max_kappa: float = 50.0

    # Training
    batch_size: int = 32
    seq_length: int = 32
    noise_levels: tuple = (0.01, 0.05, 0.1, 0.2, 0.5)

    # Langevin dynamics
    langevin_steps: int = 200
    langevin_dt: float = 0.001
    langevin_gamma: float = 0.1
    langevin_T: float = 1.0

    # SOC (Self-Organized Criticality)
    soc_threshold: float = 0.8

    # Hierarchical context
    context_local: int = 12
    context_medium: int = 64
    context_global: int = 512

    # Decoder (MoE)
    vocab_size: int = 50257
    moe_experts: int = 4
    moe_active: int = 2
    hidden_dim: int = 1024

    # Optimization
    learning_rate: float = 1e-4
    weight_decay: float = 0.01
    grad_clip: float = 1.0
üí° Training Tips
For Better Convergence
Use GPU (--device vulkan) to accelerate training
Monitor logs in real time to detect problems early
Validate checkpoints periodically with diagnose.py
Adjust curriculum learning if convergence is too slow
Verify splat statistics to ensure balanced usage
To Evaluate Quality
Use evaluate.py to calculate perplexity on WikiText-103
Generate samples from successive checkpoints to compare quality
Review energy metrics to ensure stable convergence
Verify SOC consolidation rate (should decrease over time)
üéâ Phase 1 Summary
Status: ‚úÖ Completed
New Files: 7 files improved/created
Implemented Enhancements: 5 main categories
Expected Benefits: Faster and stable convergence, real-time monitoring

Time Estimation:

Phase 1 (10 epochs): 2-3 hours on AMD RX 6650XT GPU
Full convergence: 5-7 additional days (depending on metrics)
