# EBM (Energy-Based Model) para Lenguaje

> Energy-Based Model para generaci√≥n de lenguaje sobre hiperesfera 640D, con Gaussian Splats como atractores y din√°mica Langevin para sampleo.

**Estado**: üîÑ En entrenamiento activo con Phase 1 Mejoras
**Ubicaci√≥n**: `projects/ebm/`
**Inicio**: Febrero 2026

---

## üéâ Nuevas Mejoras Implementadas (Fase 1)

### ‚úÖ Fase 1: Convergencia y Validaci√≥n

#### 1. Inicializaci√≥n Inteligente de Splats
- **Cargar embeddings GPT-2 pre-entrenadas** para representaci√≥n sem√°ntica inicial rica
- **Expandir de 10K a 50K splats** progresivamente con curriculum learning
- **Temperatura de energy configurada** para mejor exploraci√≥n inicial

**Beneficios**:
- Cobertura de vocabulario mucho mejor desde el inicio
- Representaciones iniciales semanticamente significativas
- Reducci√≥n dr√°stica del tiempo de convergencia

#### 2. Curriculum Learning
- **Fase 1 (Init)**: 5K splats, aprender representaciones b√°sicas, alta temperatura
- **Fase 2 (Mid)**: 30K splats, expandir vocabulario, temperatura media
- **Fase 3 (Max)**: 50K splats, fine-tuning completa, baja temperatura

**Beneficios**:
- Progreso m√°s predecible y estable
- Evita colapso local en m√≠nimos
- Mejor uso de capacidad GPU por fase

#### 3. Monitoreo Avanzado
- **M√©tricas en vivo**:
  - Loss score matching (por batch y por epoch)
  - Energ√≠a promedio con tendencia
  - Estad√≠sticas de splats (n_active, frecuencia, edad)
  - Tasa de consolidaci√≥n SOC
  - Perplexity en validaci√≥n WikiText-103

- **Logging detallado**:
  - Timestamps exactos por batch
  - Checkpoints cada 5 epochs (adem√°s de cada epoch)
  - Informaci√≥n de diagn√≥stico (n_splats, distancias promedio)

- **Alertas autom√°ticas**:
  - Energ√≠a aumentando inesperadamente
  - SOC consolid√°ndose demasiado r√°pido
  - Perplexity empeorando
  - Convergencia pobre detectada

#### 4. Validaci√≥n Autom√°tica
- **Evaluaci√≥n de checkpoints**:
  - Perplexity autom√°tica en subset de validaci√≥n
  - M√©tricas de energ√≠a por epoch
  - An√°lisis de tendencia de convergencia

- **Herramientas de diagn√≥stico**:
  - `diagnose.py`: An√°lisis autom√°tico de checkpoints
  - `evaluate.py`: M√©tricas de calidad generativa
  - Muestras generadas para evaluaci√≥n humana

#### 5. Mejoras de Splat Store
- **Estad√≠sticas de splats mejoradas**:
  - Seguimiento de frecuencia de uso
  - Edad de cada splat para weight decay
  - Kappa din√°mico con l√≠mites configurables (min: 1.0, max: 50.0)
  - Ajuste de temperatura para m√°s exploraci√≥n
  - Weight decay gradual por epoch

---

## üèó Arquitectura del Modelo

```
Tokenizer ‚Üí Embedding ‚Üí Splat Store ‚Üí Energy ‚Üí Langevin ‚Üí Decoder ‚Üí Tokens
                     (Œº, Œ±, Œ∫)         (Riemann)    (MoE)
```

**Componentes Mejorados**:
- **ImprovedSplatStore**: Hasta 50K splats con KNN FAISS-CPU
- **EnergyFunction**: Splat + Geom√©trica + Composicional
- **Langevin Dynamics**: Underdamped (momentum) con 200 pasos
- **SOC Controller**: Self-Organized Criticality para consolidaci√≥n
- **EBMDecoder**: Mixture of Experts (4 expertos, 2 activos)
- **Geometry**: Operaciones Riemannianas completas (exp_map, log_map, proyecci√≥n de gradientes)

---

## üöÄ C√≥mo Entrenar

### Inicio R√°pido (Vulkan GPU)

```bash
# Entrenar con mejoras de Fase 1 usando GPU
python train.py --device vulkan --epochs 10 --batch-size 32

# Reanudar desde checkpoint
python train.py --device vulkan --resume

# Validar checkpoint existente
python diagnose.py --checkpoint checkpoints/ebm_epoch_5.pt --device vulkan
```

### Diagn√≥stico Autom√°tico

```bash
# An√°lisis detallado de checkpoint espec√≠fico
python diagnose.py --checkpoint checkpoints/ebm_epoch_X.pt --device vulkan

# An√°lisis batch de todos los checkpoints
python diagnose.py --batch --device vulkan

# Generar reporte con recomendaciones
python diagnose.py --checkpoint checkpoints/ebm_epoch_10.pt --device vulkan --report
```

---

## üìä M√©tricas de √âxito

### Fase 1 Objetivos
| M√©trica | Target | Progreso |
|---------|--------|----------|
| Perplexity (WikiText) | < 100 | Pendiente de validaci√≥n |
| Energy Trend | Estable/Decreciente | Por medir en entrenamiento |
| Splat Coverage | 80%+ | Pendiente de medici√≥n |
| SOC Rate | Decreciente | Por medir en entrenamiento |

### M√©tricas de Convergencia
- **Loss Score Matching**: Target < 0.1
- **Energ√≠a Promedio**: Estable y decreciente
- **Tendencia**: Converging o Excelente (estable)
- **Tasa de Consolidaci√≥n**: Decreciendo con el tiempo

---

## üìÅ Archivos del Proyecto

### Core Architecture
- `config.py` - Configuraci√≥n centralizada (EbmConfig dataclass)
- `model.py` - EBMModel principal
- `splats.py` - ImprovedSplatStore (50K splats con KNN)
- `energy.py` - EnergyFunction (Splat + Geom√©trica + Composicional)
- `langevin.py` - Underdamped Langevin sampler
- `soc.py` - HistoryBuffer + SOC consolidation
- `decoder.py` - EBMDecoder (MoE: 4 expertos, 2 activos)
- `geometry.py` - Operaciones Riemannianas completas

### Training and Evaluation
- `train.py` - Script principal de entrenamiento con mejoras Fase 1 ‚úÖ MEJORADO
- `evaluate.py` - Evaluaci√≥n de perplexity y calidad generativa ‚úÖ NUEVO
- `diagnose.py` - Diagn√≥stico autom√°tico de checkpoints ‚úÖ NUEVO
- `pretrain.py` - Script de pretraining existente
- `train_logger.py` - Logging detallado de entrenamiento ‚úÖ NUEVO

### Utilities
- `dataset_utils.py` - WikiText-103 dataloader
- `vulkan_engine.py` - VulkanEBMRunner (GPU acceleration)
- `config.py` - Configuraci√≥n fallback

### Documentation
- `README.md` - Este archivo con instrucciones completas ‚úÖ NUEVO
- `requirements.txt` - Dependencias del proyecto ‚úÖ NUEVO

### Checkpoints and Logs
- `checkpoints/` - Model checkpoints guardados cada epoch
- `logs/ebm/` - Logs detallados de entrenamiento (JSON)

---

## üéØ Diferencias con el Dise√±o Original

| Aspecto | Original | Fase 1 Mejorado | Beneficio |
|---------|----------|-------------------|----------|
| Splats Init | 10K random | 50K GPT-2 embeddings | Mejor cobertura sem√°ntica |
| Training | Single phase | 3-phase curriculum | Convergencia m√°s estable |
| Monitoreo | B√°sico | M√©tricas en vivo + alertas | Problemas detectados temprano |
| Validaci√≥n | Manual | Autom√°tica con diagn√≥sticos | Feedback en tiempo real |
| Splat Stats | Simple | Estad√≠sticas completas | Mejor comprensi√≥n del modelo |

---

## üìñ Documentaci√≥n

### Quick Start
```bash
# Instalar dependencias
pip install -r requirements.txt

# Entrenar con GPU (Recomendado para AMD RX 6650XT)
python train.py --device vulkan --epochs 10 --batch-size 32

# Monitorear entrenamiento en vivo
# Los logs se guardan en logs/ebm/training_log_TIMESTAMP.json
```

### Diagn√≥stico
```bash
# An√°lisis de checkpoint espec√≠fico
python diagnose.py --checkpoint checkpoints/ebm_epoch_5.pt --device vulkan

# Diagn√≥stico batch de todos los checkpoints
python diagnose.py --batch --device vulkan
```

---

## üöÄ Pr√≥ximos Pasos (Fase 2 - Opcionales)

Estas mejoras solo se implementar√°n si las de Fase 1 no resuelven los problemas de convergencia:

1. **FAISS-GPU Migration**: Aceleraci√≥n real de KNN de splats
2. **Mixed Precision Training**: BF16 para 2x capacidad de batch
3. **Gradient Accumulation**: Effective batch size 8x (actualmente 1)
4. **Transformer Decoder**: Arquitectura estilo GPT-2 probada
5. **Hierarchical Sampling**: Coarse-to-fine para mayor eficiencia

---

## üîß Configuraci√≥n

### Fase 1 Par√°metros (config.py)

```python
@dataclass
class EBMConfig:
    # Ambiente
    device: str = "vulkan"  # Usar GPU AMD RX 6650XT

    # Espacio latente
    latent_dim: int = 640

    # Splats (Fase 1 mejorado)
    n_splats_init: int = 10000  # Inicial: 10K, luego expandir a 50K
    max_splats: int = 150000  # Capacidad m√°xima: 50K
    knn_k: int = 64

    # Curriculum learning (Fase 1 nuevo)
    enable_curriculum_learning: bool = True
    curriculum_epochs: int = 5
    curriculum_target_splats: int = 50000

    # Monitoreo (Fase 1 mejorado)
    enable_detailed_logging: bool = True
    soc_check_interval: int = 100

    # Regularizaci√≥n de splats (Fase 1 mejorado)
    splat_temperature: float = 0.1
    splat_weight_decay: float = 0.0
    splat_weight_decay_start: float = 1.0
    min_kappa: float = 1.0
    max_kappa: float = 50.0

    # Entrenamiento
    batch_size: int = 32
    seq_length: int = 32
    noise_levels: tuple = (0.01, 0.05, 0.1, 0.2, 0.5)

    # Din√°mica Langevin
    langevin_steps: int = 200
    langevin_dt: float = 0.001
    langevin_gamma: float = 0.1
    langevin_T: float = 1.0

    # SOC (Self-Organized Criticality)
    soc_threshold: float = 0.8

    # Hierarchical context
    context_local: int = 12
    context_medium: int = 64
    context_global: int = 512

    # Decoder (MoE)
    vocab_size: int = 50257
    moe_experts: int = 4
    moe_active: int = 2
    hidden_dim: int = 1024

    # Optimizaci√≥n
    learning_rate: float = 1e-4
    weight_decay: float = 0.01
    grad_clip: float = 1.0
```

---

## üí° Consejos de Entrenamiento

### Para Mejor Convergencia

1. **Usar GPU** (`--device vulkan`) para acelerar el entrenamiento
2. **Monitorear los logs** en tiempo real para detectar problemas temprano
3. **Validar checkpoints** peri√≥dicamente con `diagnose.py`
4. **Ajustar curriculum learning** si la convergencia es muy lenta
5. **Verificar estad√≠sticas de splats** para asegurar uso balanceado

### Para Evaluar Calidad

1. **Usar `evaluate.py`** para calcular perplexity en WikiText-103
2. **Generar muestras** de checkpoints sucesivos para comparar calidad
3. **Revisar m√©tricas de energ√≠a** para asegurar convergencia estable
4. **Verificar tasa de consolidaci√≥n SOC** (debe disminuir con el tiempo)

---

## üéâ Resumen de Fase 1

**Estado**: ‚úÖ Completado
**Archivos Nuevos**: 7 archivos mejorados/creados
**Mejoras Implementadas**: 5 categor√≠as principales
**Beneficios Esperados**: Convergencia m√°s r√°pida y estable, monitoreo en tiempo real

**Estimaci√≥n de Tiempo**:
- Fase 1 (10 epochs): 2-3 horas en GPU AMD RX 6650XT
- Convergencia completa: 5-7 d√≠as adicionales (dependiendo de m√©tricas)

---

**√öltima actualizaci√≥n**: 2026-02-21
**Autor**: Alfred üé©
